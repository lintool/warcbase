{
    "warc" : {
        # Indexing configuration:
        "index" : {
            # What to extract:
            "extract" : {
                # Maximum payload size allowed to be kept wholly in RAM:
                "inMemoryThreshold" : 10M,
                # Maximum payload size that will be serialised out to disk instead of held in RAM:
                "onDiskThreshold" : 100M,
                
                # Content to extract
                "content" : {
                    # Should we index the content body text?
                    "text" : true,
                    
                    # Should we store the text, or only index it?
                    "text_stored" : false,
                    
                    # Extract UK Postcodes and geoindex?
                    "text_extract_postcodes": true,

                    # Run simple AFINN sentiment analysis?                    
                    "test_sentimentj": false,
                     
                    # Run the Stanford NER?
                    "text_stanford_ner": false,
                    
                    # Should we extract the fuzzy hash of the text?
                    "text_fuzzy_hash" : false,
                    
                    # Extract list of elements used in HTML:
                    "elements_used" : false,
                    
                    # Extract potential PDF problems using Apache PDFBox Preflight:
                    "extractApachePreflightErrors" : false,
    
                    # Extract image features:
                    "images" : {
                        "enabled" : false,
                        "maxSizeInBytes" : 1M,
                    	"detectFaces" : false,
                    	"dominantColours" : false,
                        # The random sampling rate:
                        # (where '1' means 'extract from all images', 
                        # and '100' would mean 'extract from 1 out of every 100 images')
                        "analysisSamplingRate": 50
                    }
                    
                    # Extract the first bytes of the file (for shingling):
                    "first_bytes" : {
                        # Enabled?
                        "enabled" : false,
                        # Number of bytes to extract (>=4 to allow content_ffb to work):
                        "num_bytes" : 32
                    }
                },
                
                # Which linked entities to extract:
                "linked" : {
                    "resources" : false,
                    "hosts" : false,
                    "domains" : true
                },
                
                # Restrict record types:
                "record_type_include" : [
                    response, revisit
                ],
                
                # Restrict response codes:
                # works by matches starting with the characters, so "2" will match 2xx:
                "response_include" : [
                    "2"
                ],
                
                # Restrict protocols:
                "protocol_include" : [
                    http,
                    https
                ],
                
                # URLs to skip, e.g. ['robots.txt'] ???
                "url_exclude" : [],
            },
            
            # Parameters relating to format identification:   
            "id" : {
                # Allow tools to infer format from the resource URI (file extension):
                "useResourceURI" : true
    
                # DROID-specific config:
                "droid" : {
                    "enabled" : true,
                    "useBinarySignaturesOnly" : false
                },
            },
            
            # Parameters to control Apache Tika behaviour
            "tika" : {
                # Maximum length of text to extract:
                "max_text_length" : 2M,
                # Use the 'boilerpipe' text extractor rather than the usual one:
                "use_boilerpipe" : false,
                # The parse timeout (for when Tika gets stuck):
                "parse_timeout" : 300000,
                # Extract absolutely all metadata fields into a generic facet:
                "extract_all_metadata": false,
                # Formats to avoid processing
                "exclude_mime" : [
                    "x-tar",
                    "x-gzip",
                    "bz",
                    "lz",
                    "compress",
                    "zip",
                    "javascript",
                    "css",
                    "octet-stream"
                ]
            },
            
            # Parameters to control the exclusion of results from the indexing process:
            "exclusions" : {
                # Exclusion enabled?
                "enabled" : false,
                # Default check interval before reloading the exclusions file, in seconds:
                "check_interval" : 600,
                # Exclusion URI/SURT prefix file:
                "file" : "/path/to/exclude.txt"
            }
        },

        # Solr configuration:
        "solr" : {
            # Use the hash+url as the ID for the documents
            "use_hash_url_id": true
            # Check SOLR for duplicates during indexing:
            "check_solr_for_duplicates": false
        }
    }
}
